---
title: "Initial Results and Code"
output:
  html_document:
    df_print: paged
---

```{r Load the needed packages for SQL connection & ML, message=FALSE, warning=FALSE}
# Load the needed packages for SQL connection & ML
library("RODBC")
library("sqldf")
library("randomForest")
library("caret") #CM, Sampling, & folds
library("e1071") #SVM
```
# 1. Business Dataset
# 1.1 Data Ingestion

```{r Ingest Business data from SQL server}
# Ingest Business data from SQL server

# Yelp data comes in JSON format, SQL Sever OPEN_JSON function was used to convert it into a tabular format

# Create and open SQL connection
conn <- odbcDriverConnect('driver={SQL Server};server=.;database=CIND820_Filtered;Trusted_Connection=true;')

# Read the business (restaurant) data using the SQL connection
# Attributes such as business name and address details are ignored, since they will not add any value to this analysis
rest_data <- sqlQuery(conn, " SELECT BusinessId, Stars, ReviewCount, PriceRange, WiFi, 
                              BikeParking, BusinessParking, WheelchairAccessible, DogsAllowed, 
                              AcceptsCreditCards, AcceptsBitcoin, TableService, Delivery, TakeOut, Caters, Reservations, 
                              OutdoorSeating, GoodForGroups, Ambience, Attire, Alcohol, HappyHour, HasTV, NoiseLevel, GoodForMeal, Categories
                              FROM [CIND820_Filtered].[dbo].[BCRestaurants]"
                      )

# Convert the SQL date table into a data frames
rest_data <- as.data.frame(rest_data)
```

```{r Or: Ingest the Business data from GitHub}
# Or: Ingest the Business data from GitHub

# Alternatively, the data is also available in csv format --> part of the Github repo: https://github.com/mazinkamal134/CIND820/blob/main/Reviews.csv
rest_data = read.csv(file = "https://github.com/mazinkamal134/CIND820/raw/main/Businesses.csv", header = TRUE, na.strings = c("", "NA", "N/A", "NULL"), fileEncoding="UTF-8-BOM")

```

```{r Investigate the Restaurants DF}
# Check the structure of the DF
head(rest_data)
str(rest_data)
```
# 1.2 Data Analysis and Cleaning (1 variable at a time)

```{r 1. Restaurants.Stars}
# 1. Restaurants.Stars #########################################################

# Data is clean, no NA, normally distributed, left skewed
# Check for NA
sum(is.na(rest_data$Stars))
# Summarize
table(rest_data$Stars, useNA = "always")
# reduce the star rating to 1:5 (integers)
rest_data$Stars = as.integer(round(rest_data$Stars, 0))
# Visualize
pie(table(rest_data$Stars))
# This is our class label, and as seen the data is severely imbalanced, will deal with this imbalance later
```

```{r 2. Restaurants.ReviewCount}
# 2. Restaurants.ReviewCount ###################################################

# data is clean, no NA, right skewed
sum(is.na(rest_data$ReviewCount))
hist(rest_data$ReviewCount, xlab = "Review #",  main = "Review Count Distribution")
```

```{r 3. Restaurants.PriceRange}
#3. Restaurants.PriceRange #####################################################

# 19% of values are missing, will assign them to 1
round((sum(is.na(rest_data$PriceRange))/nrow(rest_data)) * 100, 0) 

# Summarize
table(rest_data$PriceRange, useNA = "always") # before cleaning

# Assign both "None" and NA to 1
rest_data$PriceRange[which(rest_data$PriceRange == "None" | is.na(rest_data$PriceRange))] = 1

# Summarize
table(rest_data$PriceRange, useNA = "always") # after cleaning

# Convert the datatype to int
rest_data$PriceRange = as.integer(rest_data$PriceRange)

# Check the resulting distribution
pie(table(rest_data$PriceRange))
```

```{r 4. Restaurants.WiFi}
# 4. Restaurants.WiFi ##########################################################

#23% of values are missing, will assign them to None
print("NA and None %: ")
round((sum(is.na(rest_data$WiFi))/nrow(rest_data))*100, 0)

# Summarize
print("Before cleaning")
table(rest_data$WiFi, useNA = "always") 

# Convert 'u'no', 'no' and NA to None
rest_data$WiFi[which(rest_data$WiFi == "u'no'" | rest_data$WiFi == "'no'" | is.na(rest_data$WiFi))] = "None"

# Convert 'u'free' and 'free' to Free
rest_data$WiFi[which(rest_data$WiFi == "u'free'" | rest_data$WiFi == "'free'" )] = "Free"

# Convert 'u'paid' and 'paid' to Paid
rest_data$WiFi[which(rest_data$WiFi == "u'paid'" | rest_data$WiFi == "'paid'" )] = "Paid"

# Convert to factor
rest_data$WiFi = as.factor(rest_data$WiFi)

# Summarize
print("After cleaning")
table(rest_data$WiFi, useNA = "always") 

# Visualize
pie(table(rest_data$WiFi))
```


```{r 5. Restaurants.BikeParking}
# 5. Restaurants.BikeParking ###################################################

#27% of values are missing, will assign them to False
print("NA and None %: ")
round((sum(is.na(rest_data$BikeParking))/nrow(rest_data))*100, 0) 

# Summarize
print("Before cleaning")
table(rest_data$BikeParking, useNA = "always")

# Convert NA to False
rest_data$BikeParking[is.na(rest_data$BikeParking)] = "False"

# Summarize
print("After cleaning")
table(rest_data$BikeParking, useNA = "always")

# Change the datatype to logical
rest_data$BikeParking = as.logical(rest_data$BikeParking)

# Visualize
pie(table(rest_data$BikeParking))
```

```{r 6. Restaurants.BusinessParking}
# 6. Restaurants.BusinessParking ###############################################

#13% of values are missing, will assign them to False
print("NA and None %: ")
round((sum(is.na(rest_data$BusinessParking))/nrow(rest_data))*100, 0) 

# Visualize
head(rest_data$BusinessParking)

# Need to reduce the data on this attribute to TRUE/FALSE, TRUE --> if any type of parking is available, FALSE if not
rest_data$BusinessParking[grep("True", rest_data$BusinessParking)] = "True"
rest_data$BusinessParking[-grep("True", rest_data$BusinessParking)] = "False"

# Change the data type to logical
rest_data$BusinessParking = as.logical(rest_data$BusinessParking)

# Visualize
pie(table(rest_data$BikeParking))
```

```{r 7. Restaurants.WheelchairAccessible}
# 7. Restaurants.WheelchairAccessible ##########################################

#75% of values are missing, will ignore this attribute
print("NA and None %: ")
round((sum(is.na(rest_data$WheelchairAccessible))/nrow(rest_data))*100, 0) 

# remove the unwanted attribute
rest_data = rest_data[, !names(rest_data) %in% c("WheelchairAccessible")]
```

```{r 8. Restaurants.DogsAllowed}
# 8. Restaurants.DogsAllowed ###################################################

#82% of values are missing, will ignore this attribute
print("NA and None %: ")
round((sum(is.na(rest_data$DogsAllowed))/nrow(rest_data))*100, 0) 

# remove the unwanted attribute
rest_data = rest_data[, !names(rest_data) %in% c("DogsAllowed")]
```

```{r 9. Restaurants.AcceptsCreditCards}
# 9. Restaurants.AcceptsCreditCards ############################################

#94% of values are missing, will ignore this attribute
print("NA and None %: ")
round((sum(is.na(rest_data$AcceptsCreditCards))/nrow(rest_data))*100, 0) 

# remove the unwanted attribute
rest_data = rest_data[, !names(rest_data) %in% c("AcceptsCreditCards")]
```

```{r 10. Restaurants.AcceptsBitcoin}
# 10. Restaurants.AcceptsBitcoin ###############################################

#100% of values are missing, will ignore this attribute
print("NA and None %: ")
round((sum(is.na(rest_data$AcceptsBitcoin))/nrow(rest_data))*100, 0) 

# remove the unwanted attribute
rest_data = rest_data[, !names(rest_data) %in% c("AcceptsBitcoin")]
```

```{r 11. Restaurants.TableService}
# 11. Restaurants.TableService #################################################

#59% of values are missing, will ignore this attribute
print("NA and None %: ")
round((sum(is.na(rest_data$TableService))/nrow(rest_data))*100, 0) 

# remove the unwanted attribute
rest_data = rest_data[, !names(rest_data) %in% c("TableService")]
```

```{r 12. Restaurants.Delivery}
# 12. Restaurants.Delivery #####################################################

#12% of values are missing, will assign them to False
print("NA and None %: ")
round((sum(is.na(rest_data$Delivery))/nrow(rest_data))*100, 0) 

# Summarize
print("Before cleaning")
table(rest_data$Delivery, useNA = "always")

# Convert None/NA to False
rest_data$Delivery[which(is.na(rest_data$Delivery) | rest_data$Delivery == "None")] = "False"

# Summarize
print("After cleaning")
table(rest_data$Delivery, useNA = "always")

# Change the datatype to logical
rest_data$Delivery = as.logical(rest_data$Delivery)

# Visualize
pie(table(rest_data$Delivery))
```

```{r 13. Restaurants.TakeOut}
# 13. Restaurants.TakeOut ######################################################

#9% of values are missing, will assign them to False
print("NA and None %: ")
round((sum(is.na(rest_data$TakeOut))/nrow(rest_data))*100, 0) 

# Summarize
print("Before cleaning")
table(rest_data$TakeOut, useNA = "always")

# Convert None/NA to False
rest_data$TakeOut[which(is.na(rest_data$TakeOut) | rest_data$TakeOut == "None")] = "False"

# Summarize
print("After cleaning")
table(rest_data$TakeOut, useNA = "always")

# Change the datatype to logical
rest_data$TakeOut = as.logical(rest_data$TakeOut)

# Visualize
pie(table(rest_data$TakeOut))
```

```{r 14. Restaurants.Caters}
# 14. Restaurants.Caters #######################################################

#37% of values are missing, will ignore this attribute
print("NA and None %: ")
round((sum(is.na(rest_data$Caters))/nrow(rest_data)) * 100, 0)

# remove the unwanted attribute
rest_data = rest_data[, !names(rest_data) %in% c("Caters")]
```

```{r 15. Restaurants.Reservations}
# 15. Restaurants.Reservations #################################################

#18% of values are missing, will assign them to False
print("NA and None %: ")
round((sum(is.na(rest_data$Reservations))/nrow(rest_data))*100, 0) 

# Summarize
print("Before cleaning")
table(rest_data$Reservations, useNA = "always")

# Convert None/NA to False
rest_data$Reservations[which(is.na(rest_data$Reservations) | rest_data$Reservations == "None")] = "False"

# Summarize
print("After cleaning")
table(rest_data$Reservations, useNA = "always")

# Change the datatype to logical
rest_data$Reservations = as.logical(rest_data$Reservations)

# Visualize
pie(table(rest_data$Reservations))
```

```{r 16. Restaurants.OutdoorSeating}
# 16. Restaurants.OutdoorSeating ###############################################

#19% of values are missing, will assign them to False
print("NA and None %: ")
round((sum(is.na(rest_data$OutdoorSeating))/nrow(rest_data))*100, 0) 

# Summarize
print("Before cleaning")
table(rest_data$OutdoorSeating, useNA = "always")

# Convert None/NA to False
rest_data$OutdoorSeating[which(is.na(rest_data$OutdoorSeating) | rest_data$OutdoorSeating == "None")] = "False"

# Summarize
print("After cleaning")
table(rest_data$OutdoorSeating, useNA = "always")

# Change the datatype to logical
rest_data$OutdoorSeating = as.logical(rest_data$OutdoorSeating)

# Visualize
pie(table(rest_data$OutdoorSeating))
```

```{r 17. Restaurants.GoodForGroups}
# 17. Restaurants.GoodForGroups ################################################

#22% of values are missing, will assign them to False
print("NA and None %: ")
round((sum(is.na(rest_data$GoodForGroups))/nrow(rest_data))*100, 0) 

# Summarize
print("Before cleaning")
table(rest_data$GoodForGroups, useNA = "always")

# Convert None/NA to False
rest_data$GoodForGroups[which(is.na(rest_data$GoodForGroups) | rest_data$GoodForGroups == "None")] = "False"

# Summarize
print("After cleaning")
table(rest_data$GoodForGroups, useNA = "always")

# Change the datatype to logical
rest_data$GoodForGroups = as.logical(rest_data$GoodForGroups)

# Visualize
pie(table(rest_data$GoodForGroups))
```

```{r 18. Restaurants.Ambience}
# 18. Restaurants.Ambience #####################################################

#18% of values are missing
print("NA and None %: ")
round((sum(is.na(rest_data$Ambience))/nrow(rest_data))*100, 0) 

# Visualize
head(rest_data$Ambience)

# We will ignore this attribute, since 50% of the ambiance data is missing (all JSON attributes = FALSE)
print("NA and None % - (after parsing the ambience data): ")
round(((nrow(rest_data) - length(grep("True", rest_data$Ambience)))/nrow(rest_data))*100, 0)

# remove the unwanted attribute
rest_data = rest_data[, !names(rest_data) %in% c("Ambience")]
```

```{r 19. Restaurants.Attire}
# 19. Restaurants.Attire #######################################################

#27% of values are missing, will ignore this attribute
print("NA and None %: ")
round((sum(is.na(rest_data$Attire))/nrow(rest_data))*100, 0) 

# remove the unwanted attribute
rest_data = rest_data[, !names(rest_data) %in% c("Attire")]
```

```{r 20. Restaurants.Alcohol}
# 20. Restaurants.Alcohol ######################################################

#27% of values are missing, will ignore this attribute
print("NA and None %: ")
round((sum(is.na(rest_data$Alcohol))/nrow(rest_data))*100, 0) 

# remove the unwanted attribute
rest_data = rest_data[, !names(rest_data) %in% c("Alcohol")]
```

```{r 21. Restaurants.HappyHour}
# 21. Restaurants.HappyHour ####################################################

#80% of values are missing, will ignore this attribute.
print("NA and None %: ")
round((sum(is.na(rest_data$HappyHour))/nrow(rest_data))*100, 0) 

# remove the unwanted attribute
rest_data = rest_data[, !names(rest_data) %in% c("HappyHour")]
```

```{r 22. Restaurants.HasTV}
# 22. Restaurants.HasTV ########################################################

#19% of values are missing, will assign them to False
print("NA and None %: ")
round((sum(is.na(rest_data$HasTV))/nrow(rest_data))*100, 0) 

# Summarize
table(rest_data$HasTV, useNA = "always") # before cleaning

# Convert None/NA to False
rest_data$HasTV[which(is.na(rest_data$HasTV) | rest_data$HasTV == "None")] = "False"

# Summarize
table(rest_data$HasTV, useNA = "always") # after cleaning

# Change the datatype to logical
rest_data$HasTV = as.logical(rest_data$HasTV)

# Visualize
pie(table(rest_data$HasTV))
```

```{r 23. Restaurants.NoiseLevel}
# 23. Restaurants.NoiseLevel ###################################################

#34% of values are missing, will ignore this attribute
print("NA and None %: ")
round((sum(is.na(rest_data$NoiseLevel))/nrow(rest_data))*100, 0) 

# remove the unwanted attribute
rest_data = rest_data[, !names(rest_data) %in% c("NoiseLevel")]
```

```{r 24. Restaurants.GoodForMeal}
# 24. Restaurants.GoodForMeal ##################################################

#34% of values are missing, will ignore this attribute
print("NA and None %: ")
round((sum(is.na(rest_data$GoodForMeal))/nrow(rest_data))*100, 0) 

# remove the unwanted attribute
rest_data = rest_data[, !names(rest_data) %in% c("GoodForMeal")]
```

```{r 25. Restaurants.Categories}
# 25. Restaurants.Categories ###################################################

#0% of values are missing 
print("NA and None %: ")
round((sum(is.na(rest_data$Categories))/nrow(rest_data))*100, 0) 

# will ignore this attribute, since it is part of the filter we used initially to separate the other businesses.
# remove the unwanted attribute
rest_data = rest_data[, !names(rest_data) %in% c("Categories")]
```

```{r Check the new/cleaned DF (4730 obs. of  13 variables)}
# Check the structure of the new DF
str(rest_data)
head(rest_data)
```
# 1.3 Correlation Analysis (feature selection)

```{r Check correlations (numeric attributes)}
# Check correlations (numeric attributes)

cor(rest_data[,names(rest_data) %in% c("Stars", "ReviewCount", "PriceRange")])
# there is no strong correlation between the subject attributes, they could all be included in the analysis
# PCA could have been used if there are more numeric attributes, however most of the features are TRUE/FALSE ones.
```
# 1.4 Machine Learning: Classification

```{r Convert response variable to factor}
# Support Vector Machine & Random Forest: convert the response variable to factor --> for classification
rest_data$Stars = as.factor(rest_data$Stars)
```

```{r Handle the data imbalance}
# Handle the data imbalance

# Balance the classes, using over-sampling
over_sampled_set = upSample(rest_data, rest_data$Stars)

# Check the over-sampled dataset
print("Over-sampled dataset summary:")
table(over_sampled_set$Stars)

# Balance the classes, using under-sampling
under_sampled_set = downSample(rest_data, rest_data$Stars) 

# Check the under sampled dataset
print("Under-sampled dataset summary:")
table(under_sampled_set$Stars) 

# As seen, under-sampling yields a very small sample, not fit for training, will be ignored and over-sampling will be used from now on
```
```{r Data splitting --> Training & Testing}
# split the data into training and testing sets using the over-sampled dataset
splitting_index = sample(1:nrow(over_sampled_set), 0.7*nrow(over_sampled_set))

train_set = over_sampled_set[splitting_index, ]
test_set = over_sampled_set[-splitting_index, ]
```

```{r Using Support Vector Machine & Random Forest with multiple folds}
# Using Support Vector Machine & Random Forest with multiple folds

# create multiple folds and get the accuracy per fold per model
folds = createFolds(1:nrow(over_sampled_set), k = 10)

# create a vector to hold 10 accuracy figures for each algorithm (SVM & RF)
svm_accuracies = c() 
rf_accuracies = c()

# iterate, train different models, get the model accuracy for each fold
x = 1

# loop for 10 folds
for (f in folds)
{
  print(paste("iteration: ", x))
  
  # split the data
  training_set = over_sampled_set[f, ]
  testing_set = over_sampled_set[-f, ]
  
  # Support Vector Machine model
  svm_model = svm(Stars~ReviewCount+PriceRange+WiFi+BikeParking+BusinessParking+Delivery+TakeOut+Reservations+OutdoorSeating+GoodForGroups+HasTV, data = training_set)
  svm_Prediction <- predict(svm_model, testing_set[,-1])
  svm_CM = confusionMatrix(svm_Prediction, testing_set$Stars)
  
  # Random Forest model
  rf_model = randomForest(Stars~ReviewCount+PriceRange+WiFi+BikeParking+BusinessParking+Delivery+TakeOut+Reservations+OutdoorSeating+GoodForGroups+HasTV, data = training_set)
  rf_Prediction <- predict(rf_model, testing_set[,-1])
  rf_CM = confusionMatrix(rf_Prediction, testing_set$Stars)
  
  # Accuracy vector
  svm_accuracies = c(svm_accuracies, svm_CM$overall[1])
  rf_accuracies = c(rf_accuracies, rf_CM$overall[1])
  
  # move the index forward
  x = x + 1
}

# Stats and Visualizations
print(paste("SVM mean accuracy across 10 folds (%): ", round(mean(svm_accuracies) * 100, 0)))
hist(svm_accuracies, xlab = "Accuracy", main = "SVM Model")
print(paste("Random Forest mean accuracy across 10 folds (%): ", round(mean(rf_accuracies) * 100, 0)))
hist(rf_accuracies, xlab = "Accuracy", main = "RF Model")
```

```{r Use ANOVA to check both results}
# Use ANOVA to check both results

# test which is algorithm is better using Anova 
accuracies = data.frame(accuracy = c(svm_accuracies, rf_accuracies), algorithm = as.factor(rep(1:2, rep(10, 2))))

# Visualize
summary(aov(accuracy~algorithm, data = accuracies)) 
boxplot(accuracy~algorithm, data = accuracies)

# Reject the null hypothesis --> algorithms performed differently
# Random Forest has a higher mean accuracy, so it will be used to determine the attribute importance
```

```{r Random Forest model}
# Random Forest model

# Create a new model using the full training dataset(s)
rf_model = randomForest(Stars~ReviewCount+PriceRange+WiFi+BikeParking+BusinessParking+Delivery+TakeOut+Reservations+OutdoorSeating+GoodForGroups+HasTV, data = train_set)

# use the test dataset for prediction
rf_prediction <- predict(rf_model, test_set[,-1])

# Create the confusion matrix
rf_CM = confusionMatrix(rf_prediction, test_set$Stars)

# display the confusion matrix and it's parameters
print(rf_CM)
```
# 1.5 Results

```{r Variable importance}
# Variable importance

# display the Mean Decrease Accuracy & Mean Decrease Gini --> 
# The higher the value of mean decrease accuracy or mean decrease Gini score, the higher the importance of the variable in the model.
importance(rf_model)

# Plot
varImpPlot(rf_model, main = "Random Forest Model")

# As seen, Review #, WiFi and TV availability, in addition to price range and delivery are the major contributing factors for a good star rating ... 
```

# 2. Reviews Dataset
# (data is clean, and it will be analyzed right away)

```{r Load the needed packages for text tidying and word clouds, message=FALSE, warning=FALSE}
# Load the needed packages for text tidying and word clouds
library("tidyverse")
library("dplyr")
library("tidytext")
library("wordcloud")
library("ggplot2")
```

# 2.1 Data Ingestion

```{r Ingest the Review data from SQL server}
# Ingest the Review data from SQL server

# read the review data using the SQL connection
rev_data <- sqlQuery(conn, "SELECT TOP 10000 [ReviewId], [BusinessId], [Stars], [Text] 
                            FROM dbo.BCRestaurantReviews
                            WHERE Text IS NOT NULL")
# convert the SQL date into data frames
rev_data <- as.data.frame(rev_data)

# We are reading the top 10,000 reviews only, once in production the entire dataset could be read
```

```{r Or: Ingest the Review data from GitHub}
# Ingest the Review data from GitHub

# Alternatively, the data is also available in csv format (A sample of 10K obs.) --> part of the Github repo: https://github.com/mazinkamal134/CIND820/blob/main/Reviews.csv
rev_data = read.csv(file = "https://raw.githubusercontent.com/mazinkamal134/CIND820/main/Reviews.csv", header = TRUE, na.strings = c("", "NA", "N/A", "NULL"), fileEncoding="UTF-8-BOM")
```

```{r Investigate the Reviews DF}
# check the structure of the DF
head(rev_data)
str(rev_data)
```

# 2.2 Data Tidying

```{r Review data tidying ...}
# Review data tidying ...

# display the length of the review data frame before tidying ...
print(paste("Original dataset length: ", nrow(rev_data)))

# Tokenize the review text (lower case, remove punctuation), and remove the stop words
rev_data_tidy = unnest_tokens(rev_data, word, Text) %>% anti_join(stop_words)

# Get the numbers from the tokenized dataset
nums <- rev_data_tidy %>% filter(str_detect(word, "^[0-9]")) %>% select(word) %>% unique()

# remove those numbers
rev_data_tidy = rev_data_tidy %>% anti_join(nums)

# display the length of the review data frame after tidying ...
print(paste("Tidy dataset length: ", nrow(rev_data_tidy)))

# Check
head(rev_data_tidy)
```

# 2.3 Sentiment Analysis (Lexicon based approach)

```{r Investigate the avaiable unigram lexicons in tidytext library}
# Investigate the avaiable unigram lexicons in tidytext library

# from Finn Ã…rup Nielsen
print(paste("# of words in AFINN lexicon: ", nrow(get_sentiments("afinn"))))
get_sentiments("afinn")

# from Bing Liu and collaborators
print(paste("# of words in Bing et al lexicon: ", nrow(get_sentiments("bing"))))
get_sentiments("bing")

# from Saif Mohammad and Peter Turney
print(paste("# of words in NRC lexicon: ", nrow(get_sentiments("nrc"))))
get_sentiments("nrc")

# AFINN lexicon provides the sentiment score between -5 to +5, while Bing et al lexicon classifies uni grams as negative or positive
# NRC, in addition to the word being postive or negative, provides the associated emotion. 

```

```{r Analyse sentiment using AFINN lexicon}
# Analyse sentiment using AFINN lexicon

# Piping using inner join, group by and summaries to get the final sentiment score for each review
afinn_df =  rev_data_tidy %>% 
            inner_join(get_sentiments("afinn")) %>% 
            group_by(ReviewId, BusinessId, Stars) %>% 
            summarise(sentiment = sum(value))

# Check
head(afinn_df)
```

```{r Analyse the sentiment using Bing & NRC in one shot}
# Analyse the sentiment using Bing & NRC in one shot

# Piping as before and binding both lexicon's outputs into one data frame
bing_nrc_df = bind_rows(
  (rev_data_tidy %>% inner_join(get_sentiments("bing")) %>%  mutate(Method = "Bing")),
  # ignore the emotions and filter using positive/negative sentiments
  (rev_data_tidy %>% inner_join(get_sentiments("nrc") %>%  filter(sentiment %in% c("positive", "negative"))) %>% mutate(Method = "NRC"))
) %>%
# Count and pivot based on positive and negative  
count(Method, ReviewId, BusinessId, Stars, sentiment) %>%
pivot_wider(names_from = c(Method, sentiment), values_from = n, values_fill = 0) %>% 
# Add new attributes for each lexicon output  
mutate(Bing_sentiment = Bing_positive - Bing_negative, NRC_sentiment = NRC_positive - NRC_negative) 

# Check
head(bing_nrc_df)
```

```{r Find the correlations between the overall sentiment value and the actual star rating associated with the review}
# Find the correlations between the overall sentiment value and the actual star rating associated with the review

# Bing
cor.test(bing_nrc_df$Bing_sentiment, bing_nrc_df$Stars, method = "pearson") # Bing et al lexicon performed the best (will be used for further analysis)

# AFINN
cor.test(afinn_df$sentiment, afinn_df$Stars, method = "pearson")            # Afinn lexicon performed closer to Bing

# NRC
cor.test(bing_nrc_df$NRC_sentiment, bing_nrc_df$Stars, method = "pearson")  # NRC lexicon performed the worst, will use it later for emotional analysis.
```

```{r Separate the Bing sentiments (for further analysis and visualizations)}
# Separate the Bing et al sentiments (for further analysis and visualizations)

# As before, we use piping, inner join, count, and pivoting to get the final data frame
bing_df = rev_data_tidy %>% 
          inner_join(get_sentiments("bing")) %>%
          count(ReviewId, BusinessId, Stars, sentiment) %>%
          pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
          mutate(sentiment = positive - negative, LineNum = row_number()) 

# Check
head(bing_df)
```
```{r Visualize the review counts and stars}
# Visualize the review counts and stars

# Aggregate the data
rev_data_agg =  rev_data %>% 
                group_by(Stars) %>% 
                count(name = "ReviewCount")

# Visualize
ggplot(rev_data_agg, aes(x = "", y = ReviewCount, fill = Stars)) + 
geom_bar(stat="identity", width = 1) +
coord_polar("y", start=0) +
xlab("Stars") +
ylab("Reviews #") 

# As seen, the reviews are concentrated in 3,4, and 5 stars
```

```{r Visualize the positive/negative sentiment in each star group}
# Visualize the positive/negative sentiment in each star rating group

ggplot(bing_df, aes(LineNum, sentiment, fill = Stars)) +
geom_col(show.legend = FALSE) +
xlab("Review") +
ylab("Sentiment Score") +
facet_wrap(~Stars, ncol = 2, scales = "free_x", labeller = "label_both")

# As seen, 1 and 2 star reviews tend to have negative sentiments, while 3 star reviews are kind of balanced, and 4 and 5 star reviews mostly display a positive sentiment score
```

```{r Average sentiment score across all star ratings}
# Average sentiment score across all star ratings

# The same is obvious in this visualization, both 1, and 2 star reviews have an overall/average negative sentiment
ggplot(aggregate(sentiment ~ Stars, data = bing_df, mean), aes(Stars, sentiment, fill = sentiment)) +
geom_col(show.legend = FALSE) +
xlab("Stars") +
ylab("Avg. Sentiment") +
geom_text(aes(label = round(sentiment, 2)))
```

```{r Word Clouds, warning=FALSE}
# Word Clouds

# Word clouds (starting with 1 star through 5)
par(mfrow=c(2, 3))
for (rating in 1:5) {
rev_data_tidy %>% 
    group_by(Stars, word) %>% 
    count(name = "WordCount", sort = TRUE) %>% 
    filter(Stars == rating) %>% 
    with(wordcloud(word, WordCount, max.words = 50, ordered.colors = TRUE))
}

# reset
par(mfrow=c(1, 1))

```

# 2.4 Emotional Analysis (Lexicon based approach)

```{r Investifate the NRC lexicon}
# Investifate the NRC lexicon

# Visualize the sentiment/emotions vs. # of words/unigrams in NRC lexicon
nrc_lexicon_emotions =  get_sentiments("nrc") %>% 
                        group_by(sentiment) %>% 
                        count(name = "sentimentCount")

# Plot
ggplot(nrc_lexicon_emotions, aes(sentiment, sentimentCount, fill = sentiment)) +
geom_col(show.legend = FALSE) +
xlab("Emotion") +
ylab("# of Lexicon Words") +
geom_text(aes(label = sentimentCount))
```

```{r Get the NRC emotions}
# Get the NRC emotions

# Use the tidy dataset, pipe it through inner join, filter the sentiment (positive/negative), and count
nrc_rev_emotions =  rev_data_tidy %>% 
                      inner_join(get_sentiments("nrc") %>%  
                      filter(!sentiment %in% c("positive", "negative"))) %>%
                      count(ReviewId, BusinessId, Stars, sentiment, name = "Count")
```

```{r Visualize & summarize}
# Visualize & Summarize

# use ggplot to display the overall emotions associated with each star rating
ggplot(nrc_rev_emotions, aes(sentiment, Count, fill = sentiment, label = Count)) +
geom_col(show.legend = FALSE) +
scale_x_discrete(guide = guide_axis(n.dodge=3))+
xlab("Emotion") +
ylab("# of appearances") +
facet_wrap(~Stars, ncol = 2, scales = "free_x", labeller = "label_both")
```

```{r Result}
# Results

# For ratings of 1 & 2 (low ratings or ratings associated with negative sentiments), emotions such as Joy and Trust are very low compared to 3,4, and 5 star ratings. 
# On the other hand, Joy, anticipation, and trust are noticeable across reviews associated with 3,4, and 5 star ratings, as expected. 
```





